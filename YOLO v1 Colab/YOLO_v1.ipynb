{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "YOLO v1.ipynb",
      "provenance": [],
      "toc_visible": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU"
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "H3Ghs72k0Rjt"
      },
      "source": [
        "## Download the data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "KQ5JLQftwiMj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "abf2a641-b60f-42c9-bb09-d687d8e1abfc"
      },
      "source": [
        "!mkdir train \r\n",
        "!mkdir test \r\n",
        "!wget http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar -P train/ \r\n",
        "!wget http://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar -P test/ \r\n",
        "!tar -xf test/VOCtest_06-Nov-2007.tar -C test/ \r\n",
        "!tar -xf train/VOCtrainval_06-Nov-2007.tar -C train/ \r\n",
        "!rm -rf test/VOCtest_06-Nov-2007.tar"
      ],
      "execution_count": 1,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "--2021-01-31 11:35:04--  http://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:80... connected.\n",
            "HTTP request sent, awaiting response... 301 Moved Permanently\n",
            "Location: https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar [following]\n",
            "--2021-01-31 11:35:04--  https://pjreddie.com/media/files/VOCtrainval_06-Nov-2007.tar\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 460032000 (439M) [application/octet-stream]\n",
            "Saving to: ‘train/VOCtrainval_06-Nov-2007.tar’\n",
            "\n",
            "VOCtrainval_06-Nov- 100%[===================>] 438.72M  8.86MB/s    in 63s     \n",
            "\n",
            "2021-01-31 11:36:07 (6.99 MB/s) - ‘train/VOCtrainval_06-Nov-2007.tar’ saved [460032000/460032000]\n",
            "\n",
            "URL transformed to HTTPS due to an HSTS policy\n",
            "--2021-01-31 11:36:07--  https://pjreddie.com/media/files/VOCtest_06-Nov-2007.tar\n",
            "Resolving pjreddie.com (pjreddie.com)... 128.208.4.108\n",
            "Connecting to pjreddie.com (pjreddie.com)|128.208.4.108|:443... connected.\n",
            "HTTP request sent, awaiting response... 200 OK\n",
            "Length: 451020800 (430M) [application/octet-stream]\n",
            "Saving to: ‘test/VOCtest_06-Nov-2007.tar’\n",
            "\n",
            "VOCtest_06-Nov-2007 100%[===================>] 430.13M  3.40MB/s    in 60s     \n",
            "\n",
            "2021-01-31 11:37:08 (7.15 MB/s) - ‘test/VOCtest_06-Nov-2007.tar’ saved [451020800/451020800]\n",
            "\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kCpd1WdUwszo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1d535a7a-1d89-49b2-ee74-205a41c94005"
      },
      "source": [
        "!pip install xmltodict \r\n",
        "!pip install -U albumentations"
      ],
      "execution_count": 2,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Collecting xmltodict\n",
            "  Downloading https://files.pythonhosted.org/packages/28/fd/30d5c1d3ac29ce229f6bdc40bbc20b28f716e8b363140c26eff19122d8a5/xmltodict-0.12.0-py2.py3-none-any.whl\n",
            "Installing collected packages: xmltodict\n",
            "Successfully installed xmltodict-0.12.0\n",
            "Collecting albumentations\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/03/58/63fb1d742dc42d9ba2800ea741de1f2bc6bb05548d8724aa84794042eaf2/albumentations-0.5.2-py3-none-any.whl (72kB)\n",
            "\u001b[K     |████████████████████████████████| 81kB 9.3MB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: numpy>=1.11.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.19.5)\n",
            "Requirement already satisfied, skipping upgrade: PyYAML in /usr/local/lib/python3.6/dist-packages (from albumentations) (3.13)\n",
            "Requirement already satisfied, skipping upgrade: scipy in /usr/local/lib/python3.6/dist-packages (from albumentations) (1.4.1)\n",
            "Requirement already satisfied, skipping upgrade: scikit-image>=0.16.1 in /usr/local/lib/python3.6/dist-packages (from albumentations) (0.16.2)\n",
            "Collecting imgaug>=0.4.0\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/66/b1/af3142c4a85cba6da9f4ebb5ff4e21e2616309552caca5e8acefe9840622/imgaug-0.4.0-py2.py3-none-any.whl (948kB)\n",
            "\u001b[K     |████████████████████████████████| 952kB 29.9MB/s \n",
            "\u001b[?25hCollecting opencv-python-headless>=4.1.1\n",
            "\u001b[?25l  Downloading https://files.pythonhosted.org/packages/96/fc/4da675cc522a749ebbcf85c5a63fba844b2d44c87e6f24e3fdb147df3270/opencv_python_headless-4.5.1.48-cp36-cp36m-manylinux2014_x86_64.whl (37.6MB)\n",
            "\u001b[K     |████████████████████████████████| 37.6MB 85kB/s \n",
            "\u001b[?25hRequirement already satisfied, skipping upgrade: pillow>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (7.0.0)\n",
            "Requirement already satisfied, skipping upgrade: matplotlib!=3.0.0,>=2.0.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (3.2.2)\n",
            "Requirement already satisfied, skipping upgrade: imageio>=2.3.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.4.1)\n",
            "Requirement already satisfied, skipping upgrade: networkx>=2.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (2.5)\n",
            "Requirement already satisfied, skipping upgrade: PyWavelets>=0.4.0 in /usr/local/lib/python3.6/dist-packages (from scikit-image>=0.16.1->albumentations) (1.1.1)\n",
            "Requirement already satisfied, skipping upgrade: opencv-python in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (4.1.2.30)\n",
            "Requirement already satisfied, skipping upgrade: six in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.15.0)\n",
            "Requirement already satisfied, skipping upgrade: Shapely in /usr/local/lib/python3.6/dist-packages (from imgaug>=0.4.0->albumentations) (1.7.1)\n",
            "Requirement already satisfied, skipping upgrade: pyparsing!=2.0.4,!=2.1.2,!=2.1.6,>=2.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.4.7)\n",
            "Requirement already satisfied, skipping upgrade: python-dateutil>=2.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (2.8.1)\n",
            "Requirement already satisfied, skipping upgrade: cycler>=0.10 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (0.10.0)\n",
            "Requirement already satisfied, skipping upgrade: kiwisolver>=1.0.1 in /usr/local/lib/python3.6/dist-packages (from matplotlib!=3.0.0,>=2.0.0->scikit-image>=0.16.1->albumentations) (1.3.1)\n",
            "Requirement already satisfied, skipping upgrade: decorator>=4.3.0 in /usr/local/lib/python3.6/dist-packages (from networkx>=2.0->scikit-image>=0.16.1->albumentations) (4.4.2)\n",
            "Installing collected packages: imgaug, opencv-python-headless, albumentations\n",
            "  Found existing installation: imgaug 0.2.9\n",
            "    Uninstalling imgaug-0.2.9:\n",
            "      Successfully uninstalled imgaug-0.2.9\n",
            "  Found existing installation: albumentations 0.1.12\n",
            "    Uninstalling albumentations-0.1.12:\n",
            "      Successfully uninstalled albumentations-0.1.12\n",
            "Successfully installed albumentations-0.5.2 imgaug-0.4.0 opencv-python-headless-4.5.1.48\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DkAgmtDo0guj"
      },
      "source": [
        "## Define path, classes"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NWqBbWjiyWlk"
      },
      "source": [
        "root_dir = '/content'\r\n",
        "annot_f = './{}/VOCdevkit/VOC2007/Annotations'\r\n",
        "image_f = './{}/VOCdevkit/VOC2007/JPEGImages/{}'\r\n",
        "\r\n",
        "classes = ['person', 'bird', 'cat', 'cow', 'dog', 'horse', \r\n",
        "           'sheep', 'aeroplane', 'bicycle', 'boat', 'bus', 'car', \r\n",
        "           'motorbike', 'train', 'bottle', 'chair', 'dining table', \r\n",
        "           'potted plant', 'sofa', 'tv/monitor' ]\r\n",
        "\r\n",
        "num_classes = len(classes)\r\n",
        "feature_size = 7\r\n",
        "num_bboxes = 2"
      ],
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "PdMzA-k90Ogj"
      },
      "source": [
        "## Import module"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AAWOSdSa0q-2"
      },
      "source": [
        "import sys\r\n",
        "\r\n",
        "from torch.autograd import Variable\r\n",
        "import torch\r\n",
        "import torch.nn as nn\r\n",
        "import torch.nn.functional as F\r\n",
        "import torch.optim as optim\r\n",
        "import torch.optim.lr_scheduler\r\n",
        "\r\n",
        "from torch.utils.data import Dataset, DataLoader\r\n",
        "\r\n",
        "device = torch.device('cuda:0' if torch.cuda.is_available() else 'cpu')\r\n",
        "\r\n",
        "## utils\r\n",
        "import numpy as np\r\n",
        "import random, math, time\r\n",
        "from tqdm.notebook import tqdm\r\n",
        "\r\n",
        "## File Loader\r\n",
        "import os, xmltodict\r\n",
        "import os.path as pth\r\n",
        "from PIL import Image\r\n",
        "\r\n",
        "# Draw Image\r\n",
        "import matplotlib.pyplot as plt\r\n",
        "import matplotlib.patches as patches\r\n",
        "\r\n",
        "## Transformer\r\n",
        "from random import sample\r\n",
        "import albumentations as A\r\n",
        "from albumentations.pytorch.transforms import ToTensor\r\n",
        "\r\n",
        "# Seed\r\n",
        "random.seed(53)"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "svR01qOz0xDL"
      },
      "source": [
        "## Define visualization method"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8ubA52yG13Dq"
      },
      "source": [
        "def draw_image(image_info, w=448, h=448, transforms=None):\r\n",
        "    im = np.array(Image.open(image_f.format('train', image_info['image_id'])).convert('RGB').resize((w,h)), dtype=np.uint8)\r\n",
        "\r\n",
        "    # Create figure and axes\r\n",
        "    fig, ax  = plt.subplots(1, figsize=(7,7))\r\n",
        "\r\n",
        "    bb = image_info['bboxs']\r\n",
        "    la = image_info['labels']\r\n",
        "\r\n",
        "    if transforms:\r\n",
        "        sample = transforms(image=im, bboxes=bb, category_ids=la)\r\n",
        "        im = sample['image'].permute(1,2,0).numpy()\r\n",
        "        bb = sample['bboxes']\r\n",
        "        la = sample['category_ids']\r\n",
        "\r\n",
        "    # Display the image\r\n",
        "    ax.imshow(im)\r\n",
        "\r\n",
        "    for b, l in zip(bb, la): \r\n",
        "        # top left (x, y) , (w, h)\r\n",
        "        rect = patches.Rectangle((b[0]*w,b[1]*h),(b[2]-b[0])*w,(b[3]-b[1])*h,linewidth=1,edgecolor='r',\r\n",
        "                                 facecolor='none') \r\n",
        "        # Add the patch to the Axes \r\n",
        "        ax.add_patch(rect) \r\n",
        "        props = dict(boxstyle='round', facecolor='red', alpha=0.9) \r\n",
        "        plt.text(b[0]*w, b[1]*h, classes[l], fontsize=10, color='white', bbox=props) \r\n",
        "    plt.axis('off') \r\n",
        "    plt.show()"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-60Z4Ja2AUni"
      },
      "source": [
        "## xml파일에서 정보추출하는 함수 (xmltodict라는 모듈 사용)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "bbfA0y8829lp",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "215ca4c2-ef85-4e64-b64b-273bb37bdb1a"
      },
      "source": [
        "def get_infos(annot_f=annot_f, mode='train'): \r\n",
        "    annot_dir = annot_f.format(mode) \r\n",
        "    result = [] \r\n",
        "    for ano in [pth.join(annot_dir, ano) for ano in os.listdir(annot_dir)]: \r\n",
        "        f = open(ano) \r\n",
        "        info = xmltodict.parse(f.read())['annotation'] \r\n",
        "        image_id = info['filename'] \r\n",
        "        image_size = np.asarray(tuple(map(int, info['size'].values()))[:2], np.int16) \r\n",
        "        w, h = image_size \r\n",
        "        box_objects = info['object'] \r\n",
        "        labels = [] \r\n",
        "        bboxs = [] \r\n",
        "        for obj in box_objects: \r\n",
        "            try: \r\n",
        "                labels.append(classes.index(obj['name'].lower())) \r\n",
        "                bboxs.append(tuple(map(int, obj['bndbox'].values()))) \r\n",
        "            except: pass \r\n",
        "        # Resizing Box, Change x1 y1 x2 y2 \r\n",
        "        # albumentations (normalized box) \r\n",
        "        bboxs = np.asarray(bboxs, dtype=np.float64) \r\n",
        "        try: \r\n",
        "            bboxs[:, [0,2]] /= w \r\n",
        "            bboxs[:, [1,3]] /= h \r\n",
        "        except: pass \r\n",
        "        if bboxs.shape[0] or mode=='test': \r\n",
        "            result.append({'image_id':image_id, 'image_size':image_size, 'bboxs':bboxs, 'labels':labels}) \r\n",
        "    return result \r\n",
        "    \r\n",
        "trval_list = get_infos() \r\n",
        "test_list = get_infos(mode='test') \r\n",
        "\r\n",
        "len(trval_list), len(test_list)"
      ],
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(3067, 4952)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 6
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "TdlrkJQO-STT"
      },
      "source": [
        "## split train and val data"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x62H4O3T-SXZ",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "26a29853-53db-4b07-b077-2bd62fc8fdab"
      },
      "source": [
        "def get_tv_idx(tl, k=0.5):\r\n",
        "    total_idx = range(tl) # (0,3067)\r\n",
        "    train_idx = sample(total_idx, int(tl*k)) # 3067/2 개만큼 뽑음\r\n",
        "    valid_idx = set(total_idx) - set(train_idx) # 안뽑힌 index들이 valid_idx\r\n",
        "    return train_idx, list(valid_idx)\r\n",
        "\r\n",
        "train_idx, valid_idx = get_tv_idx(len(trval_list))\r\n",
        "\r\n",
        "trval_list = np.asarray(trval_list)  # list -> array\r\n",
        "train_list = trval_list[train_idx]\r\n",
        "valid_list = trval_list[valid_idx]\r\n",
        "\r\n",
        "len(train_list), len(valid_list), len(test_list)"
      ],
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "(1533, 1534, 4952)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KdHIH0OEDKj6"
      },
      "source": [
        "## Make Dataset"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0kkeqen3DTXq"
      },
      "source": [
        "class VOCDataset(Dataset):\r\n",
        "    def __init__(self, data_list, mode='train', transforms=None):\r\n",
        "        self.data_list = data_list\r\n",
        "        self.mode = mode\r\n",
        "        self.transforms = transforms\r\n",
        "\r\n",
        "    def __len__(self):\r\n",
        "        return len(self.data_list)\r\n",
        "\r\n",
        "    def __getitem__(self, idx):\r\n",
        "        record = self.data_list[idx]\r\n",
        "        img_id = record['image_id']\r\n",
        "        bboxs = record['bboxs']\r\n",
        "        labels = record['labels']\r\n",
        "\r\n",
        "        img = Image.open(image_f.format(self.mode, img_id)).convert('RGB')\r\n",
        "        img = np.array(img)\r\n",
        "\r\n",
        "        if self.transforms:\r\n",
        "            for t in self.transforms:\r\n",
        "                sample = self.transforms(image=img, bboxes=bboxs, category_ids=labels)\r\n",
        "                image = sample['image']\r\n",
        "                bboxs = np.asarray(sample['bboxes'])\r\n",
        "                labels = np.asarray(sample['category_ids'])\r\n",
        "\r\n",
        "        if self.mode== 'train':\r\n",
        "            target = encode(bboxs, labels)\r\n",
        "            return image, target\r\n",
        "        else:\r\n",
        "            return image"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "q9a6IGzbQZGf"
      },
      "source": [
        "## (7,7,30) 에서 물체가 있는 곳에 1 대입(confidence, label)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sF8YwRNTDTjR"
      },
      "source": [
        "def encode(bboxs, labels):    \r\n",
        "    S = feature_size\r\n",
        "    B = num_bboxes\r\n",
        "    N = 5 * B + num_classes\r\n",
        "    cell_size = 1.0 / float(S)\r\n",
        "\r\n",
        "    box_cxy = (bboxs[:, 2:] + bboxs[:, :2]) / 2.0\r\n",
        "    box_wh = (bboxs[:, 2:] - bboxs[:, :2])\r\n",
        "    target = np.zeros((S,S,N))\r\n",
        "    for b in range(bboxs.shape[0]): # gt박스 수만큼 반복\r\n",
        "        cxy, wh, label = box_cxy[b], box_wh[b], labels[b]\r\n",
        "        ij = np.ceil(cxy / cell_size) -1.0 # ceil -> 소수점있으면 무조건 올림 4.1 -> 5\r\n",
        "        i,j = map(int, ij) # i,j는 셀 번호 0~6\r\n",
        "        top_left = ij*cell_size # 각 셀의 좌상단 좌표\r\n",
        "        dxy_norm = (cxy-top_left) / cell_size\r\n",
        "        \r\n",
        "        for k in range(B): # 한 셀당 두개의 박스\r\n",
        "            target[i, j, 5*k:5*(k+1)] = np.r_[dxy_norm, wh, 1]  # confidence에 1\r\n",
        "        target[j, i, 5*B+label] = 1.0 # 해당label에 1\r\n",
        "    return target\r\n"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ps6GeWXqNWjJ"
      },
      "source": [
        "## Albumentations"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "fIEH_qznQmo6"
      },
      "source": [
        "def get_train_transforms():\r\n",
        "    return A.Compose([\r\n",
        "        A.Resize(448,448, always_apply=True, p=1),\r\n",
        "        A.RandomBrightnessContrast(p=0.2),\r\n",
        "        A.HorizontalFlip(),\r\n",
        "        ToTensor(),\r\n",
        "    ],bbox_params = A.BboxParams(format='albumentations', label_fields=['category_ids']))\r\n",
        "\r\n",
        "def get_test_transforms():\r\n",
        "    return A.Compose([\r\n",
        "        A.Resize(448, 448, always_apply=True, p=1),\r\n",
        "        ToTensor(),\r\n",
        "    ])"
      ],
      "execution_count": 10,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ioswcYJiQmxm"
      },
      "source": [
        "## Data Loader"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NyvdNH24Qm0w"
      },
      "source": [
        "train_ds = VOCDataset(train_list, transforms=get_train_transforms())\r\n",
        "valid_ds = VOCDataset(valid_list, transforms=get_test_transforms())\r\n",
        "test_ds = VOCDataset(test_list, mode='test', transforms=get_test_transforms())\r\n",
        "\r\n",
        "# torch tensor를 batch size만큼 묶어줌\r\n",
        "def collate_fn(batch):\r\n",
        "    images, targets = zip(*batch)\r\n",
        "    return torch.cat([img.reshape(-1, 3, 448, 448) for img in images], 0), torch.FloatTensor(targets)\r\n",
        "\r\n",
        "def test_collate_fn(batch):\r\n",
        "    images = batch\r\n",
        "    return torch.cat([img.reshape(-1, 3, 448, 448) for img in images], 0)\r\n",
        "\r\n",
        "train_loader = DataLoader(train_ds, batch_size=32, shuffle=True, collate_fn = collate_fn)\r\n",
        "valid_loader = DataLoader(valid_ds, batch_size=32, shuffle=True, collate_fn = collate_fn)\r\n",
        "test_loader = DataLoader(test_ds, batch_size=1, shuffle=True, collate_fn = test_collate_fn)\r\n"
      ],
      "execution_count": 11,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yu5TroyslQHW"
      },
      "source": [
        "## Define YOLO v1 Model"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "q7AJ-9IPlQJ1"
      },
      "source": [
        "class YOLO_v1(nn.Module):\r\n",
        "    def __init__(self, num_classes=20, num_bboxes=2):\r\n",
        "        super(YOLO_v1, self).__init__()\r\n",
        "\r\n",
        "        self.feature_size = 7\r\n",
        "        self.num_bboxes = num_bboxes\r\n",
        "        self.num_classes = num_classes\r\n",
        "        self.conv = nn.Sequential( \r\n",
        "            nn.Conv2d(in_channels=3, out_channels=64, kernel_size=7, stride=2, padding=4), \r\n",
        "            # nn.BatchNorm2d(64), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), \r\n",
        "            \r\n",
        "            nn.Conv2d(in_channels=64, out_channels=192, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(192), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), \r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=192, out_channels=128, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(128), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=128, out_channels=256, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(256), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=256, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(256), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True),\r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), \r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(256), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(256), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(256), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=512, out_channels=256, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(256), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=256, out_channels=512, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=512, out_channels=512, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1),\r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.MaxPool2d(kernel_size=2, stride=2), \r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True),\r\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True),\r\n",
        "            nn.Conv2d(in_channels=1024, out_channels=512, kernel_size=1, stride=1, padding=0), \r\n",
        "            # nn.BatchNorm2d(512), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=512, out_channels=1024, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=2, padding=1), \r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "\r\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "            nn.Conv2d(in_channels=1024, out_channels=1024, kernel_size=3, stride=1, padding=1), \r\n",
        "            # nn.BatchNorm2d(1024), \r\n",
        "            nn.LeakyReLU(0.1, inplace=True), \r\n",
        "        )\r\n",
        "\r\n",
        "        self.fc = nn.Sequential(\r\n",
        "            Flatten(),\r\n",
        "            nn.Linear(in_features=7*7*1024, out_features=4096),\r\n",
        "            nn.LeakyReLU(0.1, inplace=True),\r\n",
        "            nn.Dropout(p=0.5),\r\n",
        "            nn.Linear(in_features=4096, out_features=(feature_size*feature_size*(5*num_bboxes+num_classes))),\r\n",
        "            nn.Softmax()\r\n",
        "        )\r\n",
        "\r\n",
        "        self.init_weight(self.conv)\r\n",
        "        self.init_weight(self.fc)\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        s,b,c = self.feature_size, self.num_bboxes, self.num_classes\r\n",
        "\r\n",
        "        x = self.conv(x)\r\n",
        "        x = self.fc(x)\r\n",
        "\r\n",
        "        x = x.view(-1, s, s, (5 * b + c))\r\n",
        "        return x\r\n",
        "\r\n",
        "    def init_weight(self, modules):\r\n",
        "        for m in modules:\r\n",
        "            if isinstance(m, nn.Conv2d):\r\n",
        "                nn.init.kaiming_normal_(m.weight, mode='fan_in', nonlinearity='leaky_relu')\r\n",
        "                if m.bias is not None:\r\n",
        "                    nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.BatchNorm2d):\r\n",
        "                nn.init.constant_(m.weight, 1)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "            elif isinstance(m, nn.Linear):\r\n",
        "                nn.init.normal_(m.weight, 0, 0.01)\r\n",
        "                nn.init.constant_(m.bias, 0)\r\n",
        "\r\n",
        "class Squeeze(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Squeeze, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return x.squeeze()\r\n",
        "\r\n",
        "\r\n",
        "class Flatten(nn.Module):\r\n",
        "    def __init__(self):\r\n",
        "        super(Flatten, self).__init__()\r\n",
        "\r\n",
        "    def forward(self, x):\r\n",
        "        return x.view(x.size(0), -1)"
      ],
      "execution_count": 12,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LLtiPIdiyoXd"
      },
      "source": [
        "# Inter section over union"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "MaPD1rEWlQLt"
      },
      "source": [
        "def compute_iou(bbox1, bbox2):\r\n",
        "    '''\r\n",
        "    Compute the IOU (Intersection over Union) of two set of bboxes, each bbox format: [x1,y1,x2,y2]\r\n",
        "    :param bbox1: (Tensor) bounding boxes, sized [N,4]\r\n",
        "    :param bbox2: (Tensor) bounding boxes, sized [N,4]\r\n",
        "    :return: (Tensor) IoU, sized [N, M].\r\n",
        "    '''\r\n",
        "\r\n",
        "    N = bbox1.size(0)\r\n",
        "    M = bbox2.size(0)\r\n",
        "\r\n",
        "    # Compute left-top coordinate of the intersections\r\n",
        "    lt = torch.max(\r\n",
        "        bbox1[:, :2].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\r\n",
        "        bbox2[:, :2].unsqueeze(0).expand(N, M, 2)  # [M,2] -> [1,M,2] -> [N,M,2]\r\n",
        "    )\r\n",
        "    # Compute right-bottom coordinate of the intersections\r\n",
        "    rb = torch.min(\r\n",
        "        bbox1[:, 2:].unsqueeze(1).expand(N, M, 2),  # [N,2] -> [N,1,2] -> [N,M,2]\r\n",
        "        bbox2[:, 2:].unsqueeze(0).expand(N, M, 2)  # [M,2] -> [1,M,2] -> [N,M,2]\r\n",
        "    )\r\n",
        "    # Compute area of the intersections from the coordinates\r\n",
        "    wh = rb - lt  # width and height of the intersection, [N,M,2]\r\n",
        "    wh[wh < 0] = 0  # clip at 0\r\n",
        "    inter = wh[:, :, 0] * wh[:, :, 1]  # [N,M]\r\n",
        "\r\n",
        "    # Compute area of the bboxes\r\n",
        "    area1 = (bbox1[:, 2] - bbox1[:, 0]) * (bbox1[:, 3] - bbox1[:, 1])  # [N,]\r\n",
        "    area2 = (bbox2[:, 2] - bbox2[:, 0]) * (bbox2[:, 3] - bbox2[:, 1])  # [N,]\r\n",
        "    area1 = area1.unsqueeze(1).expand_as(inter)  # [N,] -> [N, 1] -> [N,M]\r\n",
        "    area2 = area2.unsqueeze(0).expand_as(inter)  # [N,] -> [N, 1] -> [N,M]\r\n",
        "\r\n",
        "    # Compute IoU from the areas\r\n",
        "    union = area1 + area2 - inter\r\n",
        "    iou = inter / union\r\n",
        "\r\n",
        "    return iou"
      ],
      "execution_count": 13,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "DjYT0qORzoUa"
      },
      "source": [
        "## Loss function"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "1OBD_hJ2lQOX"
      },
      "source": [
        "def loss_fn(pred_tensor, target_tensor):\r\n",
        "    \"\"\" Compute loss for YOLO training.\r\n",
        "    Args:\r\n",
        "        pred_tensor: (Tensor) predictions, sized [n_batch, S, S, Bx5+C], 5=len([x, y, w, h, conf]).\r\n",
        "        target_tensor: (Tensor) targets, sized [n_batch, S, S, Bx5+C].\r\n",
        "    Returns:\r\n",
        "        (Tensor): loss, sized [1, ].\r\n",
        "    \"\"\"\r\n",
        "    S, B, C = feature_size, num_bboxes, num_classes\r\n",
        "    N = 5 * B + C  # 5=len([x, y, w, h, conf]\r\n",
        "    lambda_coord = 5\r\n",
        "    lambda_noobj = 0.5\r\n",
        "\r\n",
        "    batch_size = pred_tensor.size(0)\r\n",
        "    coord_mask = target_tensor[:, :, :, 4] > 0  # mask for the cells which contain objects. [n_batch, S, S]\r\n",
        "    noobj_mask = target_tensor[:, :, :, 4] == 0 # mask for the cells which not contain objects [n_batch, S, S]\r\n",
        "    coord_mask = coord_mask.unsqueeze(-1).expand_as(target_tensor)  # [n_batch, S, S] -> [n_batch, S, S, N]\r\n",
        "    noobj_mask = noobj_mask.unsqueeze(-1).expand_as(target_tensor)  # [n_batch, S, S] -> [n_batch, S, S, N]\r\n",
        "\r\n",
        "    coord_pred = pred_tensor[coord_mask].view(-1, N)  # pred tensor on the cells which contain objects. [n_coord, N]\r\n",
        "                                                        # n_coord: numver of the cells which contain objects.\r\n",
        "    bbox_pred = coord_pred[:, :5*B].contiguous().view(-1, 5)  # [n_coord x B, 5=len([x, y, w, h, conf])]\r\n",
        "    class_pred = coord_pred[:, 5*B:]                          # [n_coord, C]\r\n",
        "\r\n",
        "    coord_target = target_tensor[coord_mask].view(-1, N)  # target tensor on the cells which contain objects. [n_coord, N]\r\n",
        "\r\n",
        "    bbox_target = coord_target[:, :5*B].contiguous().view(-1, 5) # [batch*7*7*2, 5]\r\n",
        "    class_target = coord_target[:, 5*B:]\r\n",
        "\r\n",
        "    # Compute loss for the cells with no object bbox\r\n",
        "\r\n",
        "    noobj_pred = pred_tensor[noobj_mask].view(-1, N)  # [n_noobj, N]\r\n",
        "    noobj_target = target_tensor[noobj_mask].view(-1, N)\r\n",
        "\r\n",
        "    noobj_conf_mask = torch.cuda.ByteTensor(noobj_pred.size()).fill_(0)  # [n_noobj, N]\r\n",
        "    for b in range(B):\r\n",
        "        noobj_conf_mask[:, 4+B*5] = 1 # noobj_conf_mask[:, 4] = 1; noobj_conf_mask[:, 9] = 1\r\n",
        "    noobj_pred_conf = noobj_pred[noobj_conf_mask]  # [n_noobj, 2=len([conf1, conf2])]\r\n",
        "    noobj_target_conf = noobj_target[noobj_conf_mask]\r\n",
        "    # No object confidence loss (SSE)\r\n",
        "    loss_noobj = F.mse_loss(noobj_pred_conf, noobj_target_conf, reduction='sum')\r\n",
        "\r\n",
        "    # Compute loss for the cells with objects\r\n",
        "    coord_response_mask = torch.cuda.ByteTensor(bbox_target.size()).fill_(0)  # [n_coord x B, 5]\r\n",
        "    coord_not_response_mask = torch.cuda.ByteTensor(bbox_target.size()).fill_(1)  # [n_coord x B, 5]\r\n",
        "    bbox_target_iou = torch.zeros(bbox_target.size()).cuda()  # [n_coord x B, 5], only the last 1=(conf,) is used\r\n",
        "\r\n",
        "    # Choose the predicted bbox having the highest IoU for each target bbox\r\n",
        "    for i in range(0, bbox_target.size(0), B):\r\n",
        "        pred = bbox_pred[i:i + B]  # predicted bboxes at i-th cell, [B, 5=len([x, y, w, h, conf])]\r\n",
        "        pred_xyxy = Variable(torch.FloatTensor(pred.size()))  # [B, 5=len([x1, y1, x2, y2, conf])]\r\n",
        "        # Because (center_x,center_y)=pred[:, 2] and (w,h)=pred[:,2:4] are normalized for cell-size and image-size respectively,\r\n",
        "        # rescale (center_x,center_y) for the image-size to compute IoU correctly.\r\n",
        "        pred_xyxy[:, :2] = pred[:, :2] / float(S) - 0.5 * pred[:, 2:4]\r\n",
        "        pred_xyxy[:, 2:4] = pred[:, :2] / float(S) + 0.5 * pred[:, 2:4]\r\n",
        "\r\n",
        "        target = bbox_target[\r\n",
        "            i]  # target bbox at i-th cell. Because target boxes contained by each cell are identical in current implementation, enough to extract the first one.\r\n",
        "        target = bbox_target[i].view(-1, 5)  # target bbox at i-th cell, [1, 5=len([x, y, w, h, conf])]\r\n",
        "        target_xyxy = Variable(torch.FloatTensor(target.size()))  # [1, 5=len([x1, y1, x2, y2, conf])]\r\n",
        "        # Because (center_x,center_y)=target[:, 2] and (w,h)=target[:,2:4] are normalized for cell-size and image-size respectively,\r\n",
        "        # rescale (center_x,center_y) for the image-size to compute IoU correctly.\r\n",
        "        target_xyxy[:, :2] = target[:, :2] / float(S) - 0.5 * target[:, 2:4]\r\n",
        "        target_xyxy[:, 2:4] = target[:, :2] / float(S) + 0.5 * target[:, 2:4]\r\n",
        "\r\n",
        "        iou = compute_iou(pred_xyxy[:, :4], target_xyxy[:, :4])  # [B, 1]\r\n",
        "        max_iou, max_index = iou.max(0)\r\n",
        "        max_index = max_index.data.cuda()\r\n",
        "\r\n",
        "        coord_response_mask[i + max_index] = 1\r\n",
        "        coord_not_response_mask[i + max_index] = 0\r\n",
        "\r\n",
        "        # \"we want the confidence score to equal the intersection over union (IOU) between the predicted box and the ground truth\"\r\n",
        "        # from the original paper of YOLO.\r\n",
        "        bbox_target_iou[i + max_index, torch.LongTensor([4]).cuda()] = (max_iou).data.cuda()\r\n",
        "    bbox_target_iou = Variable(bbox_target_iou).cuda()\r\n",
        "\r\n",
        "    # BBox location/size and objectness loss for the response bboxes.\r\n",
        "    bbox_pred_response = bbox_pred[coord_response_mask].view(-1, 5)  # [n_response, 5]\r\n",
        "    bbox_target_response = bbox_target[coord_response_mask].view(-1,\r\n",
        "                                                                    5)  # [n_response, 5], only the first 4=(x, y, w, h) are used\r\n",
        "    target_iou = bbox_target_iou[coord_response_mask].view(-1,\r\n",
        "                                                            5)  # [n_response, 5], only the last 1=(conf,) is used\r\n",
        "    loss_xy = F.mse_loss(bbox_pred_response[:, :2], bbox_target_response[:, :2], reduction='sum')\r\n",
        "    loss_wh = F.mse_loss(torch.sqrt(bbox_pred_response[:, 2:4]), torch.sqrt(bbox_target_response[:, 2:4]),\r\n",
        "                            reduction='sum')\r\n",
        "    loss_obj = F.mse_loss(bbox_pred_response[:, 4], target_iou[:, 4], reduction='sum')\r\n",
        "\r\n",
        "    # Class probability loss for the cells which contain objects.\r\n",
        "    loss_class = F.mse_loss(class_pred, class_target, reduction='sum')\r\n",
        "\r\n",
        "    # Total loss\r\n",
        "    loss = lambda_coord * (loss_xy + loss_wh) + loss_obj + lambda_noobj * loss_noobj + loss_class\r\n",
        "    loss = loss / float(batch_size)\r\n",
        "\r\n",
        "    return loss"
      ],
      "execution_count": 14,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "h8m6DDhgttbn"
      },
      "source": [
        "## optimizer and Learning rate scheduler"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "dE1fqPg7lQSV"
      },
      "source": [
        "yolo = YOLO_v1().cuda()\r\n",
        "\r\n",
        "init_lr = 0.001\r\n",
        "base_lr = 0.01\r\n",
        "optimizer = optim.SGD(yolo.parameters(), lr=init_lr, momentum=0.9, weight_decay=5e-4)\r\n",
        "\r\n",
        "def update_lr(optimizer, epoch, burnin_base, burnin_exp=4.0):\r\n",
        "    if epoch in range(50):\r\n",
        "        lr = init_lr + (base_lr - init_lr) * math.pow(epoch/(50-1), burnin_exp)\r\n",
        "    elif epoch == 50:\r\n",
        "        lr = base_lr\r\n",
        "    elif epoch == 750:\r\n",
        "        lr = 0.001\r\n",
        "    elif epoch == 1050:\r\n",
        "        lr = 0.0001\r\n",
        "    else:\r\n",
        "        return\r\n",
        "\r\n",
        "    for param_group in optimizer.param_groups:\r\n",
        "        param_group['lr'] = lr"
      ],
      "execution_count": 15,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yFbBTsG_lQUo"
      },
      "source": [
        "## Training"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Ne537BzNlQXF"
      },
      "source": [
        "start_time = time.time()\r\n",
        "bl = len(train_loader) # 48\r\n",
        "history = {'total_loss':[]}\r\n",
        "for epoch in range(150):\r\n",
        "    tk0 = tqdm(train_loader, total=bl, leave=False)\r\n",
        "    t_loss = 0.0\r\n",
        "    breaking=False\r\n",
        "\r\n",
        "    for step, (image, target) in enumerate(tk0):\r\n",
        "        image, target = image.to(device), target.to(device)\r\n",
        "        update_lr(optimizer, epoch, float(step) / float(bl - 1))\r\n",
        "        output = yolo(image)\r\n",
        "        loss = loss_fn(output, target).cuda()\r\n",
        "\r\n",
        "        if math.isnan(loss):\r\n",
        "            print(loss)\r\n",
        "            breaking = True\r\n",
        "            break\r\n",
        "        optimizer.zero_grad()\r\n",
        "        loss.backward()\r\n",
        "        optimizer.step()\r\n",
        "        t_loss += loss.item()\r\n",
        "\r\n",
        "        history['total_loss'].append(loss.item())\r\n",
        "\r\n",
        "    if breaking:\r\n",
        "        break\r\n",
        "\r\n",
        "    # print statistics\r\n",
        "    tqdm.write(f'[EPOCH : {epoch+1} total_loss: {t_loss/bl} Total_elapesd_time: {(time.time()-start_time)/60}분')\r\n",
        "\r\n",
        "    state = {'epoch': epoch,\r\n",
        "             'model': yolo,\r\n",
        "             'optimizer': optimizer}\r\n",
        "    filename = 'checkpoint_yolov1.pth.tar'\r\n",
        "    torch.save(state, filename)\r\n",
        "\r\n",
        "print(time.time() - start_time)\r\n",
        "print('Finished Training')\r\n"
      ],
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WlRWYGr0XsX8"
      },
      "source": [
        "## 예측결과 Tensor를 다시 boxes, labels, confidences, class_score로 decoding하는 함수"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ8tKNjMs8yB"
      },
      "source": [
        "def decode(pred_tensor):\r\n",
        "    '''\r\n",
        "    Decode tensor into box coordinates, class labels, and probs_detected.\r\n",
        "    Args:\r\n",
        "        pred_tensor: (tensor) tensor to decode sized [S, S, 5 x B + C], 5=(x, y, w, h, conf)\r\n",
        "    Returns:\r\n",
        "        boxes: (tensor) [[x1, y1, x2, y2]_obj, ...]. Normalized from 0.0 to 1.0 w.r.t. image width/height, sized [n_boxes, 4].\r\n",
        "        labels: (tensor) class labels for each detected box, sized [n_boxes]\r\n",
        "        confidences: (tensor) objectness confidences for each detected box, sized [n_boxes].\r\n",
        "        class_score: (tensor) scores for most likely class for each detected box, sized [n_boxes,].\r\n",
        "    '''    \r\n",
        "    S,B,C = feature_size, num_bboxes, num_classes\r\n",
        "    conf_thresh = 0.1\r\n",
        "    prob_thresh = 0.1\r\n",
        "    nms_thresh = 0.5\r\n",
        "\r\n",
        "    boxes, labels, confidences, class_scores = [], [], [], []\r\n",
        "\r\n",
        "    cell_size = 1.0 / float(S)\r\n",
        "\r\n",
        "    conf = pred_tensor[:,:,4].unsqueeze(2)  # [7, 7, 1]\r\n",
        "    for b in range(1,B):\r\n",
        "        conf = torch.cat((conf, pred_tensor[:,:,5*b+4].unsqueeze(2)), 2)\r\n",
        "    conf_mask = conf > conf_thresh # [S, S, B]\r\n",
        "\r\n",
        "    # TBM, further optimization may be possible by replacing thre following for-loops with tensor opterations.\r\n",
        "    for i in range(S): # for x-dimension.\r\n",
        "        for j in range(S): # for y_dimension.\r\n",
        "            class_score, class_label = torch.max(pred_tensor[j, i, 5*B:], 0) # 해당 셀에서 가장 높은 class score\r\n",
        "            \r\n",
        "            for b in range(B):\r\n",
        "                conf = pred_tensor[j, i, 5*b+4]\r\n",
        "                prob = conf * class_score\r\n",
        "                if float(prob) < prob_thresh:\r\n",
        "                    continue\r\n",
        "                \r\n",
        "                # Compute box corner (x1, y1, x2, y2) from tensor.\r\n",
        "                box = pred_tensor[j, i, 5*b : 5*b+4]\r\n",
        "                # cell left-top corner. Normalized from 0.0 to 1.0 w.r.t. image width/height\r\n",
        "                x0y0_normalized = torch.FloatTensor([i,j]) * cell_size \r\n",
        "                # box center. Normalized from 0.0 to 1.0 w.r.t. image width/height.\r\n",
        "                xy_normalized = box[:2] * cell_size + x0y0_normalized\r\n",
        "                # Box width and height. Normalized from 0.0 to 1.0 w.r.t. image width/height.\r\n",
        "                wh_normalized = box[2:]\r\n",
        "\r\n",
        "                box_xyxy = torch.FloatTensor(4) # [4,]\r\n",
        "                box_xyxy[:2] = xy_normalized - 0.5 * wh_normalized # left-top corner(x1, y1)\r\n",
        "                box_xyxy[2:] = xy_normalized + 0.5 * wh_normalized # right-bottom corner(x2, y2)\r\n",
        "\r\n",
        "                # Append result to the lists\r\n",
        "                boxes.append(box_xyxy)\r\n",
        "                labels.append(class_label)\r\n",
        "                confidences.append(conf)\r\n",
        "                class_score.append(class_score)\r\n",
        "\r\n",
        "    if len(boxes) > 0:\r\n",
        "        boxes = torch.stack(boxes, 0) # [n_boxes, 4]\r\n",
        "        labels = torch.stack(labels, 0) # [n_boxes]\r\n",
        "        confidences = torch.stack(confidences, 0) # [n_boxes]\r\n",
        "        class_score = torch.stack(class_score, 0) # [n_boxes]\r\n",
        "    else:\r\n",
        "        # If no box found, return empty tensors.\r\n",
        "        boxes = torch.FloatTensor(0, 4) \r\n",
        "        labels = torch.LongTensor(0) \r\n",
        "        confidences = torch.FloatTensor(0) \r\n",
        "        class_scores = torch.FloatTensor(0)\r\n",
        "\r\n",
        "    return boxes, labels, confidences, class_score"
      ],
      "execution_count": 24,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Brc6H8gXMdbV"
      },
      "source": [
        "## Test Visualization"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "d0uJlV6BMde3"
      },
      "source": [
        "def test_visualize(images, outputs):\r\n",
        "    fig, ax = plt.subplots(1, figsize=(7,7))\r\n",
        "    img = Image.open(image_f.format('test',test_list[0]['image_id']))\r\n",
        "    w,h = test_list[0]['image_size']\r\n",
        "    im = np.asarray(img)\r\n",
        "\r\n",
        "    ax.imshow(im)\r\n",
        "\r\n",
        "    for output in outputs:\r\n",
        "        b, l, c, sc = decode(output)\r\n",
        "        if b.shape[0]: \r\n",
        "            # patches.Rectangle(xy,width,height)\r\n",
        "            rect = patches.Rectangle((b[0]*w,b[1]*h),(b[2]-b[0])*w,(b[3]-b[1])*h,linewidth=1,edgecolor='r',facecolor='none') \r\n",
        "            ax.add_path(rect) \r\n",
        "            probs = dict(boxstyle='round', facecolor='red', alpha=.9) \r\n",
        "            plt.text(b[0]*w, b[1]*h, '%s : %.2f'%(classes[l], sc), fontsize=10, color='white', bbox=props)\r\n"
      ],
      "execution_count": 31,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0uCqgQ8rZ-IT"
      },
      "source": [
        ""
      ],
      "execution_count": null,
      "outputs": []
    }
  ]
}